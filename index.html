<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="google-site-verification" content="DykGAn8-hiUCZwQR_LqQ1V-Rg8UOhvXmI32I8HTY81s" />
<!--<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />-->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<!--<meta  name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=0.5, maximum-scale=2.0, user-scalable=yes"/>-->
<meta http-equiv="Content-Type" content="text/html; charset=gb2312"/>
<link rel="stylesheet" href="Files/jemdoc.css" type="text/css" />

<link rel="shortcut icon" href="./Files/favicon.ico">
<title>Zhuo Su</title>
</head>
 


<body> 




<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<h1>Zhuo Su 苏 卓</h1></A>
<table class="imgtable"><tr><td>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="./"><img src="./Files/suzhuo.png" alt="" height="320px" /></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
<td align="left"><p><a href="./"></a><br />
<i> 
I am a senior researcher at Bytedance. 
Previously, I served as a senior researcher at Tencent (Recruitment Talents Program: "技术大咖 ").
<!-- Before that, I earned my Master's degree from Department of Automation, -->
Before that, I graduated from the Department of Automation, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, supervised by <a href="https://media.au.tsinghua.edu.cn/info/1009/1112.htm">Prof. Qionghai Dai</a> and
<a href="http://luvision.net/">Prof. Lu Fang</a>, and meanwhile, I worked closely with <a href="https://liuyebin.com/">Prof. Yebin Liu</a> and <a href="https://www.xu-lan.com/index.html">Lan Xu</a>.
<br />
<br />

My work mission is to capture and understand dynamic human-centric scenes in the real world, and digitalize humans, objects, and events for immersive application in virtual and augmented reality.
My research primarily centers around computer vision and graphics, especially human 3D generation, avatar creation, 4D reconstruction, neural rendering, motion capture, and related areas. 
<br />
<br />

<!-- <b style="color: #FF7F7F;">I am looking for full-time partners and research interns, please feel free to drop me an email if you are interested in the topics above.</b> -->
<span style="color: #749ce8;">I am looking for full-time partners and research interns, please feel free to drop me an email if you are interested in the topics above!</span>





</a></i>
<br />
<br />
Email: suzhuo13@gmail.com | suzhuo@bytedance.com
<br />
<br />
<class="staffshortcut">
 <!-- <A HREF="#Background">Background</A> |  -->
 <!-- <A HREF="#Interests">Interests</A> |  -->
 <A HREF="#Research">Research</A> | 
 <A HREF="#Awards">Awards</A>|
 <A HREF="#Skills">Skills</A> |
 <A HREF="#Services">Services</A> |
 <!--<a href="./Files/cv_zhuosu.pdf">CV</a>-->
 <A HREF=" https://scholar.google.com/citations?user=iaqDkqMAAAAJ&hl">Google Scholar</A>
<br />
<br />
 

</td></tr></table>


 

<!-- <A NAME="Background"><h2>Background</h2></A>
<ul>
    <font style="line-height:1.8;">
        <b>M.S.</b>,&nbsp Department of Automation, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, Beijing, China.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2018.08-2021.06 &nbsp;&nbsp;&nbsp;&nbsp;
        <br />
        GPA: 3.89/4.0 (GPA ranking: 5/137)
        <br />
        <b>B.E.</b>,&nbsp Department of Automation, <a href="http://english.neu.edu.cn/">Northeastern University</a>, Shenyang, China.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2014.09-2018.06 &nbsp;&nbsp;&nbsp;&nbsp;
        <br />
        GPA: 4.18/5.0 (GPA ranking: 5/276; Comprehensive ranking: 1/276)
    </font>
</span></ul>
<br /> -->


 
<!-- <A NAME="Interests"><h2>Interests</h2></A>
    &nbsp;&nbsp;&nbsp;&nbsp; My research interests include computer vision and comptuter graphics. Currently, I focus on the following topics:
<ul>
<li>Human Performance/Motion Capture</li>
<li>Photorealistic 3D Modeling/Rendering</li>
<Li>Reconstruction of Dynamic Scenes</Li>
</ul>
<br /> -->




<A NAME="Research"><h2>Research</h2></A>
<font size="3"> 

<class="staffshortcut">
 <A HREF="#Generation">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3D Generation / Avatar Creation</A>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 <!-- <A HREF="#Avatars"> Avatar Creation</A> |  -->
 <A HREF="#Reconstruction">Performance Capture / Neural Rendering</A>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 <!-- <A HREF="#Rendering">Neural Rendering</A> | -->
 <A HREF="#Motion Capture">Motion Capture / Generation</A> 
<br />
<ul>


    <A NAME="Generation"></A><h3>3D Generation / Avatar Creation </h3></A>


    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/humansplat.jpg" alt="" height="190px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors</b>
            <br />
            <a>Panwang Pan*</a>, <b>Zhuo Su*</b> (Project Lead), <a>Chenguo Lin*</a>, <a>Zhen Fan</a>, <a>Yongjie Zhang</a>, <a>Zeming Li</a>, <a>Tingting Shen</a>, <a>Yadong Mu</a>, <a>Yebin Liu</a> (*Equal Contribution)
            <br />
            <b><i> Neural Information Processing Systems Annual Conference (NeurIPS), 2024.</i></b>
            <br /><br />
            We propose HumanSplat, a method that predicts the 3D Gaussian Splatting properties of a human from a single input image in a generalizable way. It utilizes a 2D multi-view diffusion model and a latent reconstruction transformer with human structure priors to effectively integrate geometric priors and semantic features.
            <br />

            [<a href="https://arxiv.org/pdf/2406.12459">Paper</a>]
            [<a href="https://humansplat.github.io/">Project page</a>]     

         </a></span>
         </p>
    </td></tr></table> 
    

    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/joint2human.png" alt="" height="140px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Joint2Human: High-quality 3D Human Generation via Compact Spherical Embedding of 3D Joints</b>
            <br />
            <a>Muxin Zhang</a>, <a>Qiao Feng</a>, <b>Zhuo Su</b>, <a>Chao Wen</a>, <a>Zhou Xue</a>, <a>Kun Li</a>
            <br />
            <b><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024.</i></b>
            <br /><br />
             We introduce Joint2Human, a novel method that leverages 2D diffusion models to generate detailed 3D human geometry directly, ensuring both global structure and local details. 
            <br />

            [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Joint2Human_High-Quality_3D_Human_Generation_via_Compact_Spherical_Embedding_of_CVPR_2024_paper.pdf">Paper</a>]
            [<a href="https://cic.tju.edu.cn/faculty/likun/projects/Joint2Human/index.html">Project page</a>]     

         </a></span>
         </p>
    </td></tr></table> 



<!-- 
    <A NAME="Avatars"></A><h3>2. Avatar Creation</h3></A> -->

    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/ohta.png" alt="" height="132px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OHTA: One-shot Hand Avatar via Data-driven Implicit Priors</b>
            <br />
            <a>Xiaozheng Zheng</a>, <a>Chao Wen</a>, <b>Zhuo Su</b>, <a>Zeran Xu</a>, <a>Zhaohu Li</a>, <a>Yang Zhao</a>, <a>Zhou Xue</a>
            <br />
            <b><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024.</i></b>
            <br /><br />
             OHTA is a novel approach capable of creating implicit animatable hand avatars using just a single image. It facilitates 1) text-to-avatar conversion, 2) hand texture and geometry editing, and 3) interpolation and sampling within the latent space.
            <br />
            [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_OHTA_One-shot_Hand_Avatar_via_Data-driven_Implicit_Priors_CVPR_2024_paper.pdf">Paper</a>]
            [<a href="https://github.com/bytedance/OHTA">Project page</a>]  

         </a></span>
         </p>
    </td></tr></table> 



    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/headgap.png" alt="" height="102px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors</b>
            <br />
            <a>Xiaozheng Zheng</a>, <a>Chao Wen</a>, <a>Zhaohu Li</a>, <a>Weiyi Zhang</a>, <b>Zhuo Su</b>, <a>Xu Chang</a>, <a>Yang Zhao</a>, <a>Zheng Lv</a>, <a>Xiaoyuan Zhang</a>, <a>Yongjie Zhang</a>, <a>Guidong Wang</a>, <a>Lan Xu</a>
            <br />
            <b><i>International Conference on 3D Vision (3DV), 2025</i></b>
            <br /><br />
            We propose a 3D head avatar creation method that generalizes from few-shot in-the-wild data. By using 3D head priors from a large-scale dataset and a Gaussian Splatting-based network, our approach achieves high-fidelity rendering and robust animation.
            <br />
            [<a href="https://arxiv.org/pdf/2408.06019v1">Paper</a>]
            [<a href="https://headgap.github.io/">Project page</a>]  

         </a></span>
         </p>
    </td></tr></table> 



    <A NAME="Reconstruction"></A><h3>Performance Capture / Neural Rendering</h3></A>




    <!-- <A NAME="Rendering"></A><h3>4. Neural Rendering</h3></A> -->

    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/Hifi4G.png" alt="" height="108px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting</b>
            <br />
            <a>Yuheng Jiang</a>, <a>Zhehao Shen</a>, <a>Penghao Wang</a>, <b> Zhuo Su</b>, <a>Yu Hong</a>, <a>Yingliang Zhang</a>, <a>Jingyi Yu</a>, <a>Lan Xu</a>            
            <br />
            <b><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024.</i></b>
            <br /><br />
             we present an explicit and compact Gaussian-based approach for high-fidelity human performance rendering from dense footage, in which our core intuition is to marry the 3D Gaussian representation with non-rigid tracking. 
            <br />
            [<a href="https://arxiv.org/pdf/2312.03461.pdf">Paper</a>]
            [<a href="https://nowheretrix.github.io/HiFi4G/">Project page</a>]

         </a></span>
         </p>
    </td></tr></table> 


    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/instant_nvr.png" alt="" height="134px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Instant-NVR: Instant Neural Volumetric Rendering for Human-object Interactions from Monocular RGBD Stream</b>
            <br />
            <a>Yuheng Jiang</a>, <a>Kaixin Yao</a>, <b>Zhuo Su</b>, <a>Zhehao Shen</a>, <a>Haimin Luo</a>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>
            <br />
            <b><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.</i></b>
            <br /><br />
             We propose a neural approach for instant volumetric human-object tracking and rendering using a single RGBD camera. It bridges traditional non-rigid tracking with recent instant radiance field techniques via a multi-thread tracking-rendering mechanism. 
            <br />
            [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Instant-NVR_Instant_Neural_Volumetric_Rendering_for_Human-Object_Interactions_From_Monocular_CVPR_2023_paper.pdf">Paper</a>]
            [<a href="https://nowheretrix.github.io/Instant-NVR/">Project page</a>]

         </a></span>
         </p>
    </td></tr></table>    


    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/robustfusionplus.jpg" alt="" height="193px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Robust Volumetric Performance Reconstruction under Human-object Interactions from Monocular RGBD Stream</b>
            <br />
            <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>, <a>Dawei Zhong</a>, <a>Zhong Li</a>, <a>Fan Deng</a>, <a>Shuxue Quan</a>, <a href="http://luvision.net/">Lu Fang</a>
            <br />
            <b><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022.</i></b>
            <br /><br />
            <!-- We propose UnstructuredFusion, which allows realtime, high-quality, complete reconstruction of 4D textured models of human performance via only three commercial RGBD cameras. -->
            We propose a robust volumetric performance reconstruction system for human-object interaction scenarios using only a single RGBD sensor, which combines various data-driven visual and interaction cues to handle the complex interaction patterns and severe occlusions. 
            <br />
            [<a href="./Projects/RobustFusion_plus_page/RobustFusionPlus.pdf">Paper</a>]
            [<a href="./Projects/RobustFusion_plus_page/index.html">Project page</a>]
        
    </a></span>
    </p>
    </td></tr></table>


    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/neuralrendering.jpg" alt="" height="138px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NeuralHOFusion: Neural Volumetric Rendering Under Human-Object Interactions</b>
            <br />
            <a>Yuheng Jiang</a>, <a>Suyi Jiang</a>, <a>Guoxing Sun</a>, <b>Zhuo Su</b>, <a>Kaiwen Guo</a>, <a>Minye Wu</a>, <a>Jingyi Yu</a>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>
            <br />
            <b><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</i></b>
            <br /><br />
            We propose a robust neural volumetric rendering method for human-object interaction scenarios using 6 RGBD cameras, which achieves layer-wise and photorealistic reconstruction results of human performance in novel views.
            <br />
            [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_NeuralHOFusion_Neural_Volumetric_Rendering_Under_Human-Object_Interactions_CVPR_2022_paper.pdf">Paper</a>]
            [<a href="https://nowheretrix.github.io/neuralfusion/">Project page</a>]
        
    </a></span>
    </p>
    </td></tr></table>

    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/robustfusion.jpg" alt="" height="180px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
    
            <b>
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RobustFusion: Human Volumetric Capture with Data-driven Visual Cues using a RGBD Camera
                <br />
            </b>
            <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>, <a href="https://zhengzerong.github.io/">Zerong Zheng</a>, <a href="https://ytrock.com/">Tao Yu</a>, <a href="https://liuyebin.com/">Yebin Liu</a>, <a href="http://luvision.net/">
            Lu Fang</a>
            <br />
            <b><i>European Conference on Computer Vision (ECCV), 2020, Spotlight.</i></b>
            <br /><br />
            We introduce a robust human volumetric capture approach combined with various data-driven visual cues using a Kinect, which outperforms existing state-of-the-art approaches significantly.
            <br />
            [<a href="./Files/Rofusion.pdf">Paper</a>]
            [<a href="./Projects/RobustFusion_page/index.html">Project page</a>]
        </a></span>
        </p>
        </td></tr></table>


    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/unfusion.jpg" alt="" height="150px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
    
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UnstructuredFusion: Realtime 4D Geometry and Texture Reconstruction using Commercial RGBD Cameras</b>
            <br />
            <a href="https://www.xu-lan.com/index.html">Lan Xu</a>, <b>Zhuo Su</b>, <a href="https://sites.google.com/view/lhanaf/%E9%A6%96%E9%A1%B5">Lei Han</a>, <a href="https://ytrock.com/">Tao Yu</a>, <a href="https://liuyebin.com/">Yebin Liu</a>, <a href="http://luvision.net/">Lu Fang</a>
            <br />
            <b><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2019.</i></b>
            <br /><br />
            We propose UnstructuredFusion, which allows realtime, high-quality, complete reconstruction of 4D textured models of human performance via only three commercial RGBD cameras.
            <br />
            [<a href="./Files/Unfusion.pdf">Paper</a>]
            [<a href="./Projects/UnstructedFusion_page/index.html">Project page</a>]
    
    </a></span>
    </p>
    </td></tr></table>
 


    

    <A NAME="Motion Capture"></A><h3>Motion Capture / Motion Generation</h3></A>


    
    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/SMGDiff.png" alt="" height="140px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SMGDiff: Soccer Motion Generation using diffusion probabilistic models
            </b>
            <br />
            <a>Hongdi Yang</a>, <a>Chengyang Li</a>, <a>Zhenxuan Wu</a>, <a>Gaozheng Li</a>, <a>Jingya Wang</a>, <a>Jingyi Yu</a>, <b>Zhuo Su</b>, <a>Lan Xu</a>
            <br />
            <b><i>Arxiv, 2024.</i></b>
            <br /><br />
            We introduce SMGDiff, a novel two-stage framework for generating real-time and user-controllable soccer motions. Our key idea is to integrate real-time character control with a powerful diffusion-based generative model, ensuring high-quality and diverse output motion.
            <br />
            [<a href="https://arxiv.org/pdf/2411.16216">Paper</a>]
            [<a>Project page: Coming soon</a>]

         </a></span>
         </p>
    </td></tr></table> 


    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/envposer.png" alt="" height="158px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;EnvPoser: Environment-aware Realistic Human Motion Estimation from Sparse Observations with Uncertainty Modeling
            </b>
            <br />
            <a>Songpengcheng Xia</a>, <a>PYu Zhang</a>, <b>Zhuo Su</b>(Project Lead), <a>Xiaozheng Zheng</a>, <a>Zheng Lv</a>, <a>Guidong Wang</a>, <a>Yongjie Zhang</a>, <a>Qi Wu</a>, <a>Lei Chu</a>, <a>Ling Pei</a>  (*Equal Contribution)         
            <br />
            <b><i>Arxiv, 2024.</i></b>
            <br /><br />
            We propose EnvPoser, a two-stage method using sparse tracking signals and pre-scanned environment from VR devices to perform full-body motion estimation and handle the multi-hypothesis nature with uncertainty-aware and environmental constraint integration.
            <br />
            [<a href="https://arxiv.org/pdf/2412.10235">Paper</a>]
            [<a>Project page: Coming soon</a>]

         </a></span>
         </p>
    </td></tr></table> 


    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/EMHI.png" alt="" height="164px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;EMHI: A Multimodal Egocentric Human Motion Dataset with HMD and Body-Worn IMUs
            </b>
            <br />
            <a>Zhen Fan*</a>, <a>Peng Dai*</a>, <b>Zhuo Su*</b>, <a>Xu Gao</a>, <a>Zheng Lv</a>, <a>Jiarui Zhang</a>, <a>Tianyuan Du</a>, <a>Guidong Wang</a>, <a>Yang Zhang</a>  (*Equal Contribution)         
            <br />
            <b><i>Annual AAAI Conference on Artificial Intelligence (AAAI), 2025.</i></b>
            <br /><br />
            We introduce EMHI, a dataset combining stereo images from headsets and IMU data for egocentric human motion capture in VR. It includes 28.5 hours of data from 58 subjects. We also propose MEPoser, a method that effectively uses this multimodal data for improved pose estimation.
            <br />
            [<a href="https://arxiv.org/pdf/2408.17168">Paper</a>]
            [<a>Project page: Coming soon</a>]

         </a></span>
         </p>
    </td></tr></table> 




    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/hmdposer.png" alt="" height="157px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HMD-Poser: On-Device Real-time Human Motion Tracking from Scalable Sparse Observations</b>
            <br />
            <a>Peng Dai</a>, <a>Yang Zhang</a>, <a>Tao Liu</a>, <a>Zhen Fan</a>, <a>Tianyuan Du</a>, <b>Zhuo Su</b>, <a>Xiaozheng Zheng</a>, <a>Zeming Li</a>           
            <br />
            <b><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024.</i></b>
            <br /><br />
            We propose HMD-Poser, the first unified approach torecover full-body motions using scalable sparse observations from HMD and body-worn IMUs. In particular, it can support a variety of input scenarios, such as HMD,HMD+2IMUs, HMD+3IMUs, etc. 
            <br />
            [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Dai_HMD-Poser_On-Device_Real-time_Human_Motion_Tracking_from_Scalable_Sparse_Observations_CVPR_2024_paper.pdf">Paper</a>]
            [<a href="https://pico-ai-team.github.io/hmd-poser">Project page</a>]

         </a></span>
         </p>
    </td></tr></table> 



    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/AvatarJLM.png" alt="" height="108px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Realistic Full-Body Tracking from Sparse Observations via Joint-Level Modeling</b>
            <br />
            <a>Xiaozheng Zheng*</a>, <b>Zhuo Su*</b>, <a>Chao Wen</a>, <a>Zhou Xue</a>, <a>Xiaojie Jin</a> (*Equal Contribution)
            <br />
            <b><i>IEEE/CVF International Conference on Computer Vision (ICCV), 2023.</i></b>
            <br /><br />
             We propose a two-stage framework that can obtain accurate and smooth full-body motions with the three tracking signals of head and hands only, in which we first explicitly model the joint-level features and then utilize them as spatiotemporal transformer tokens to capture joint-level correlations. 
            <br />
            [<a href="https://arxiv.org/pdf/2308.08855.pdf">Paper</a>]
            [<a href="https://github.com/zxz267/AvatarJLM/">Project page</a>]

         </a></span>
         </p>
    </td></tr></table>  


    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/stylemotion.png" alt="" height="134px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Learning Variational Motion Prior for Video-based Motion Capture</b>
            <br />
            <a>Xin Chen*</a>, <b>Zhuo Su*</b>, <a>Lingbo Yang*</a>, <a>Pei Cheng</a>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>, <a>Gang Yu</a> (*Equal Contribution)
            <br />
            <b><i>arXiv, 2022.</i></b>
            <br /><br />
            We propose a novel variational motion prior (VMP) learning approach for video-based motion capture. Specifically, VMP is implemented as a transformer-based variational autoencoder pretrained over large-scale 3D motion data, providing an expressive latent space for human motion at sequence level. 
            <br />
            [<a href="https://arxiv.org/pdf/2210.15134.pdf">Paper</a>]
         </a></span>
         </p>
    </td></tr></table>




</ul>
<br />
 

 

<A NAME="Projects"><h2>Patents</h2></A>
<font size="3"> 
    <ul>
    <li> <a href="http://luvision.net/">Lu Fang</a>,<a>Mengqi Ji</a>, <a href="https://sites.google.com/view/lhanaf/%E9%A6%96%E9%A1%B5">Lei Han</a>, <b>Zhuo Su</b>,  <a href="http://media.au.tsinghua.edu.cn/english/team/qhdai.html"> Qionghai Dai</a>,
        “Depth Camera-based three-dimensional renconstruction method and apparatus, device, and storage medium”, Patent, US201916977899A
   </li>
    <li> <a href="http://luvision.net/">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://sites.google.com/view/lhanaf/%E9%A6%96%E9%A1%B5">Lei Han</a>, <a href="http://media.au.tsinghua.edu.cn/english/team/qhdai.html"> Qionghai Dai</a>,
         “Depth camera calibration method and device, electronic equipment and storage medium”, Patent, CN:201810179738:A
    </li>
    <li>
        <a href="http://luvision.net/">Lu Fang</a>, <a href="https://sites.google.com/view/lhanaf/%E9%A6%96%E9%A1%B5">Lei Han</a>, <b>Zhuo Su</b>, <a href="http://media.au.tsinghua.edu.cn/english/team/qhdai.html"> Qionghai Dai</a>,
        “A three-dimensional rebuilding method and device based on a depth camera, an apparatus and a storage medium”, Patent, CN:201810179264:A
    </li>
    <li>
        <a href="http://luvision.net/">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>,
        “Dynamic three-dimensional reconstruction method, device, equipment, medium and system”, Patent, CN:201910110062:A
    </li>
    <li>
        <a href="http://luvision.net/">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>,
        “Texture real-time determination method, device and equipment for dynamic scene and medium”, Patent, CN:201910110044:A
    </li>
    <li>
        <a href="http://luvision.net/">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>, Jianwei Wen, Chao Yuan,
        “Dynamic human body three-dimensional reconstruction method, device, equipment and medium”, Patent, CN:202010838902:A
    </li>
    <li>
        <a href="http://luvision.net/">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>, Jianwei Wen, Chao Yuan,
        “Dynamic human body three-dimensional model completion method and device, equipment and medium”, Patent, CN:202010838890:A
    </li>
    <li>
        <b>Zhuo Su</b>, Xiaozhe Wang, Wen Fei, Changfu Zhou,
        “Multi-feature information landmark detection method for precise landing of unmanned aerial vehicle”, Patent, CN:201710197369:A
    </li>
    <li> Wen Fei, <b>Zhuo Su*</b> (*corresponding author), Changfu Zhou,
         “Artificial landmark design and detection using hierarchy information for UAV localization and landing”,
    Conference Paper, Chinese Control And Decision Conference (CCDC) 2017,
        <!-- [<a href="./Files/UAV.pdf">Paper</a>] -->
    </li>
    <li>Haina Wu, <b>Zhuo Su</b>, Kai Luo, Qi Wang, Xianzhong Cheng
        "Exploration and Research on the Movement of Magnus Glider”, Journal Paper, Physical Experiment of College, 2015 (5): 2
        <!--[<a href="./Files/Glider.pdf">Paper</a>]-->
    </li>
</ul>
<br />


<!-- awards -->
<A NAME="Awards"><h2>Awards</h2></A>
<font size="3"> 
<ul>
<li>Outstanding Graduate of Beijing, Beijing, 2021</li>
<li>Outstanding Graduate of Department of Automation, Tsinghua University, 2021</li>
<li>Excellent Bachelor Thesis Award, Northeastern University, 2018</li>
<li>Outstanding Graduate of Liaoning Province, Liaoning Province, 2018</li>
<li>National Scholarship, Ministry of Education, 2018</li>
<li>Excellence Award for National Undergraduate Innovation Program, Northeastern University, 2017</li>
<li>City's Excellent Undergraduate, Shenyang City, 2017</li>
<li>Mayor's Scholarship, Shenyang City, 2017</li>
<li>Top Ten Excellent Undergraduate (10 / the whole university, 十佳本科生), Northeastern University, 2017</li>
<li>Honorable Mention of American Mathematical Contest in Modeling, COMAP, 2017</li>
<li>Second Prize of National Undergraduate Mathematical Contest in Modeling, CSIAM, 2016</li>
<li>First Prize of Provincial Undergraduate Mathematical Contest in Modeling, Liaoning Province, 2016</li>
<li>2x Second Prize of Electronic Design Contest, Education Department of Liaoning Province, 2015-2016</li>
<li>4x First Class Scholarships, Northeastern University, 2015-2018</li>
</ul>
</font>
<br />

<A NAME="Skills"><h2>Skills</h2></A>
&nbsp;&nbsp;&nbsp;&nbsp; C & C++(OpenCV, OpenGL, CUDA, Eigen, ...), Python(Pytorch), Matlab, LaTeX, ...
<br />

<A NAME="Services"><h2>Services</h2></A>
&nbsp;&nbsp;&nbsp;&nbsp; Reviewer for NeurIPS, CVPR, ICCV, ICLR, ICML, TVCG, IEEEVR, 3DV, AISTATS, ...
<br />


<br />

<!--<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5vmure15sa4&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
-->
</body>
</html>
