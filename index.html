<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">


<head>
<meta name="google-site-verification" content="DykGAn8-hiUCZwQR_LqQ1V-Rg8UOhvXmI32I8HTY81s" />
<!--<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />-->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<!--<meta  name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=0.5, maximum-scale=2.0, user-scalable=yes"/>-->
<meta http-equiv="Content-Type" content="text/html; charset=gb2312"/>
<link rel="stylesheet" href="Files/jemdoc.css" type="text/css" />

<link rel="shortcut icon" href="./Files/favicon.ico">
<title>Zhuo Su</title>


<style>
    .paper-list {
        list-style-type: none; /* 去掉默认的点 */
        padding: 0; /* 移除默认的内边距 */
        margin: 0; /* 移除默认的外边距 */
    }

    .paper-item {
        margin-bottom: 20px; /* 给每个论文条目增加一些间距 */
    }
    .staffshortcut {
    display: flex;
    justify-content: center;  /* 居中 */
    align-items: center;
    gap: 15px;  /* 增加选项间距 */
    padding: 10px 0;
    font-size: 16px;
    font-weight: 500;
    }

    .staffshortcut a {
        text-decoration: none;
        color: #333;  /* 默认颜色 */
        padding: 8px 12px;
        transition: all 0.3s ease;
        position: relative;
    }

    .staffshortcut a::after {
        content: "|";
        color: #999;
        margin-left: 15px; /* 间距 */
    }

    .staffshortcut a:last-child::after {
        content: ""; /* 最后一个不加 | */
    }

    .staffshortcut a:hover {
        color: #007bff;  /* 悬停变蓝 */
        font-weight: 600;
        transform: scale(1.1); /* 微放大 */
    }
    .staffshortcut {
    display: flex;
    justify-content: center;
    align-items: center;
    flex-wrap: nowrap; /* 禁止换行 */
    gap: 10px; /* 减少间距，避免挤压 */
    padding: 10px 15px; /* 适当缩小 padding */
    font-size: 16px;
    font-weight: 500;
    background: linear-gradient(90deg, #e3f2fd, #bbdefb); /* 渐变蓝 */
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    white-space: nowrap; /* 防止文本换行 */
    max-width: 100%; /* 确保不会超出父容器 */
    overflow-x: auto; /* 如果超出宽度，允许横向滚动 */
    }

    a:hover {
    background-color: #f1f1f1;
    color: #007bff;
    border-color: #0056b3;
    }


</style>

</head>
 

<body> 

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<h1 style="font-size: 2em; text-align: center;">Zhuo Su | 苏卓</h1>



<table class="imgtable"><tr><td>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="./"><img src="./Files/suzhuo.jpg" alt="" height="240px" /></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
<td align="left"><p><a href="./"></a>
<i>

<p>
    I am currently a Tech Lead and Researcher at ByteDance.
    My work mission is to capture and understand human-centric scenes in the real world, and digitalize humans, objects, and events for immersive applications in VR / AR.
    Prior to this, I joined Tencent as a Senior Researcher through the Special Recruitment Talents Program (<span style="font-style: italic;">技术大咖&nbsp</span>). 
    Before entering the industry, I graduated from the Department of Automation at Tsinghua University, where I had the honor of being supervised by <a href="https://media.au.tsinghua.edu.cn/info/1009/1112.htm"style="color: #007bff;">Qionghai Dai</a> and
    <a href="http://luvision.net/" style="color: #007bff;">Lu Fang</a>, and collaborated closely with <a href="https://liuyebin.com/" style="color: #007bff;">Yebin Liu</a> and <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>.
</p>

<!-- <p>
    My work mission is to capture and understand human-centric scenes in the real world, and digitalize humans, objects, and events for immersive applications in VR/AR.
</p> -->




<!-- My work mission is to capture and understand dynamic human-centric scenes in the real world, and digitalize humans, objects, and events for immersive application in virtual and augmented reality. -->
<!-- My research primarily centers around computer vision and graphics, especially human 3D generation, avatar creation, 4D reconstruction, neural rendering, motion capture, and related areas.  -->


<!-- <b style="color: #FF7F7F;">I am looking for full-time partners and research interns, please feel free to drop me an email if you are interested in the topics above.</b> -->
<!-- <span style="color: #749ce8;">I am looking for full-time partners and research interns, please feel free to drop me an email if you are interested in the topics above!</span> -->


<!-- <br /> -->
<div style="text-align: center; font-size: 1.1em;">
    <p style="text-align: left; font-size: 1em; margin-bottom: 10px;">
        Email: 
        <a href="mailto:suzhuo13@gmail.com" style="text-decoration: none; color: #0077cc;">suzhuo13@gmail.com</a> 
        | 
        <a href="mailto:suzhuo@bytedance.com" style="text-decoration: none; color: #0077cc;">suzhuo@bytedance.com</a>
    </p>
    <!-- <br /> -->
    <div class="staffshortcut" style="margin-top: 10px;">
        <a href="#Research">Research</a>
        <a href="#Talks">Talks</a>
        <a href="#Awards">Awards</a>
        <a href="#Miscellaneous">Miscellaneous</a> 
        <a href="https://scholar.google.com/citations?user=iaqDkqMAAAAJ&hl">Google Scholar</a>
    </div>
</div>
 

</td></tr></table>


 
<!-- <A NAME="Background"><h2>Background</h2></A>
<ul>
    <font style="line-height:1.8;">
        <b>M.S.</b>,&nbsp Department of Automation, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, Beijing, China.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2018.08-2021.06 &nbsp;&nbsp;&nbsp;&nbsp;
        <br />
        GPA: 3.89/4.0 (GPA ranking: 5/137)
        <br />
        <b>B.E.</b>,&nbsp Department of Automation, <a href="http://english.neu.edu.cn/">Northeastern University</a>, Shenyang, China.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2014.09-2018.06 &nbsp;&nbsp;&nbsp;&nbsp;
        <br />
        GPA: 4.18/5.0 (GPA ranking: 5/276; Comprehensive ranking: 1/276)
    </font>
</span></ul>
<br /> -->


 
<!-- <A NAME="Interests"><h2>Interests</h2></A>
    &nbsp;&nbsp;&nbsp;&nbsp; My research interests include computer vision and comptuter graphics. Currently, I focus on the following topics:
<ul>
<li>Human Performance/Motion Capture</li>
<li>Photorealistic 3D Modeling/Rendering</li>
<Li>Reconstruction of Dynamic Scenes</Li>
</ul>
<br /> -->




<A NAME="Research"><h2 style="font-size: 1.6em;">Research</h2></A>
<!-- Interests and Services Section -->
<div style="display: flex; flex-wrap: wrap; justify-content: space-between; gap: 20px; margin-bottom: 0px; padding: 0 15px;">
    <div style="flex: 1; min-width: 240px; max-width: 20%; padding: 20px; box-sizing: border-box;">
        <h3 style="margin-bottom: 0px; font-size: 1.2em;">Interests</h3>
        <ul style="list-style-type: disc; padding-left: 20px; margin-bottom: 0px;">
            <li>Avatar Creation</li>
            <li>AIGC (3D/Motion)</li>
            <li>Performance Capture</li>
            <li>Neural Rendering</li>
            <li>Motion Capture</li>
        </ul>
    </div>
    <div style="flex: 1; min-width: 240px; max-width: 70%; padding: 20px; box-sizing: border-box;">
        <h3 style="margin-bottom: 0px; font-size: 1.2em;">Services</h3>
        <ul style="list-style-type: disc; padding-left: 0; margin-bottom: 0px;">
            <li>Conference Reviewer: CVPR, ICCV, NeurIPS, ICLR, ICML, AAAI, AAAI-AIA, SIGGRAPH ASIA, IEEE VR, 3DV, ACMMM, WACV, AISTATS, ...</li>
            <li>Journal Reviewer: IEEE TVCG, TMLR</li>
            <li>Program Committee Member: AAAI 2026 Conference, AAAI 2026 AI Alignment Track</li>
            <li>Workshop Organizer: <a href="https://s2025.conference-schedule.org/presentation/?id=twork_102&sess=sess278" style="color: hsl(0, 0%, 0%)";>SIGGRAPH 2025</a>  </li>
        </ul>
        <h3 style="margin-bottom: 0px; font-size: 1.2em;">Skills</h3>
        <ul style="list-style-type: disc; padding-left: 0; margin-bottom: 0;">
            <li>Python (Pytorch, ...), C & C++ (OpenGL, CUDA, ...), Matlab, LaTeX, ...</li>
        </ul>
    </div>
</div>
<!-- Call to Action Section -->
<div style="padding: 0 30px; text-align: left; margin-bottom: 20px;">
    <h3 style="margin-bottom: 0px; font-size: 1.2em; font-weight: bold;">Let's Collaborate!</h3>
    <li>Open to discussions from academia and industry on tech implementation and real-world impact.</li>
    <li>Looking for full-time collaborators and research interns. Contact me via email if interested.</li>

</div>

<!-- Publication Section -->
<div style="padding: 0 30px; text-align: center; margin-bottom: 20px;">
    <h3 style="font-size: 1.2em;">Publication</h3>
</div>
<nav class="staffshortcut">
    <a href="#" class="filter-btn" data-filter="all">All</a>
    <a href="#" class="filter-btn" data-filter="generation">Generation</a>
    <a href="#" class="filter-btn" data-filter="animatable-avatar">Animatable Avatar</a>
    <a href="#" class="filter-btn" data-filter="performance-capture">Performance Capture</a>
    <a href="#" class="filter-btn" data-filter="neural-rendering">Neural Rendering</a>
    <a href="#" class="filter-btn" data-filter="motion-capture">Motion Capture</a>
</nav>

<br />
<!-- 用一条淡灰色横线分隔，减少突兀感 -->
<div style="border-top: 1px solid #ddd; margin-top: 10px; padding-top: 5px; text-align: center; font-size: 16px; color: #666;">
    * denotes Equal contribution, † denotes Project lead, ‡ denotes Corresponding author
</div>
<br />

<script>
    document.addEventListener("DOMContentLoaded", function () {
        const filterButtons = document.querySelectorAll(".filter-btn");
        const paperItems = document.querySelectorAll(".paper-item");
    
        filterButtons.forEach(button => {
            button.addEventListener("click", function (event) {
                event.preventDefault();
                const category = this.getAttribute("data-filter");
    
                paperItems.forEach(item => {
                    if (category === "all" || item.getAttribute("data-category").includes(category)) {
                        item.style.display = "list-item"; // 显示匹配的论文
                    } else {
                        item.style.display = "none"; // 隐藏不匹配的论文
                    }
                });
            });
        });
    });
</script>


<!-- 论文列表 -->
<ul class="paper-list">

    <li class="paper-item" data-category="performance-capture neural-rendering">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="./"><img src="./Files/TaoGS.png" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>Topology-Aware Optimization of Gaussian Primitives for Human-Centric Volumetric Videos</b>
                        <br />
                        <span style="font-size: 14px; color: #444;">Yuheng Jiang, Chengcheng Guo, Yize Wu, Yu Hong, Shengkun Zhu, Zhehao Shen, Yingliang Zhang, Shaohui Jiao , <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>, <a href="https://people.mpi-inf.mpg.de/~mhaberma/" style="color: #007bff;">Marc Habermann</a>, <a href="https://people.mpi-inf.mpg.de/~theobalt/" style="color: #007bff;">Christian Theobalt</a></span>
                        <br />
                        <span style="font-size: 14px; color: #444;"><i><b>SIGGRAPH Asia 2025</b></i></span>
                        <br />
                        <p style="font-size: 14px; color: #444;">
                           We propose TaoGS, a framework that disentangles motion and appearance for coherent volumetric video under topological variation, with codec-friendly compression for scalable high-fidelity rendering.
                        </p>
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="https://arxiv.org/pdf/2509.07653" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="https://guochch.github.io/TaoGS/" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />
                </td>
            </tr>
        </table>
    </li>


    <li class="paper-item" data-category="generation motion-capture">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="./"><img src="./Files/SMGDiff.png" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>SMGDiff: Soccer Motion Generation using Diffusion Probabilistic Models</b>
                        <br />
                        <span style="font-size: 14px; color: #444;">Hongdi Yang, Chengyang Li, Zhenxuan Wu, Gaozheng Li, Jingya Wang, <a href="https://scholar.google.com/citations?user=R9L_AfQAAAAJ&hl" style="color: #007bff";>Jingyi Yu</a>, <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a></span>
                        <br />
                        <span style="font-size: 14px; color: #444;"><i><b>ICCV 2025</b></i></span>
                        <br />
                        <p style="font-size: 14px; color: #444;">
                            We introduce SMGDiff, a novel two-stage framework for generating real-time and user-controllable soccer motions. Our key idea is to integrate real-time character control with a powerful diffusion-based generative model, ensuring high-quality and diverse output motion.
                        </p>
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="https://arxiv.org/pdf/2411.16216" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="https://geekyoung.red/SMGDiff/" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />
                </td>
            </tr>
        </table>
    </li>


    <li class="paper-item" data-category="animatable-avatar neural-rendering">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="./"><img src="./Files/PGHM.png" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling</b>
                        <br />
                        <span style="font-size: 14px; color: #444;">Cheng Peng, Jingxiang Sun, Yushuo Chen, Zhaoqi Su, <b>Zhuo Su</b>, <a href="https://www.liuyebin.com/" style="color: #007bff";>Yebin Liu</a> </span>
                        <br />
                        <span style="font-size: 14px; color: #444;"><i><b>arXiv 2025</b></i></span>
                        <br />
                        <p style="font-size: 14px; color: #444;">
                        We present PGHM, a generalizable and efficient framework that integrates human priors into 3DGS for fast and high-fidelity avatar reconstruction from monocular videos.
                        </p>
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="https://arxiv.org/pdf/2506.06645?" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="https://arxiv.org/pdf/2506.06645?" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />
                </td>
            </tr>
        </table>
    </li>


    <li class="paper-item" data-category="animatable-avatar neural-rendering">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="./"><img src="./Files/SEGA.png" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>SEGA: Drivable 3D Gaussian Head Avatar from a Single Image</b>
                        <br />
                        <span style="font-size: 14px; color: #444;">Chen Guo*, <b>Zhuo Su*‡</b> <span style="color: #888;">(Corresponding Author)</span>, <a href="https://jianwang-mpi.github.io/" style="color: #007bff;">Jian Wang</a> , Shuang Li, Xu Chang, Zhaohu Li, Yang Zhao, Guidong Wang, <a href="https://rqhuang88.github.io/" style="color: #007bff;">Ruqi Huang‡</a> </span>
                        <!-- <span style="font-size: 14px; color: #444;">Songpengcheng Xia, Yu Zhang, <b>Zhuo Su† </b> <span style="color: #888;">(Project Lead)</span>, Xiaozheng Zheng, Zheng Lv, Guidong Wang, Yongjie Zhang, Qi Wu, Lei Chu, Ling Pei</span> -->
                        <br />
                        <span style="font-size: 14px; color: #444;"><i><b>arXiv 2025</b></i></span>
                        <br />
                        <p style="font-size: 14px; color: #444;">
                            We propose SEGA, a novel approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that combines generalized prior models with a new hierarchical UV-space Gaussian Splatting framework.
                        </p>
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="https://arxiv.org/pdf/2504.14373" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="https://sega-head.github.io/" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />
                </td>
            </tr>
        </table>
    </li>


    <li class="paper-item" data-category="motion-capture">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="./"><img src="./Files/envposer.png" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>EnvPoser: Environment-aware Realistic Human Motion Estimation from Sparse Observations with Uncertainty Modeling</b>
                        <br />
                        <span style="font-size: 14px; color: #444;"> <a href="https://xspc.github.io/"style="color: #007bff;"> Songpengcheng Xia</a>, Yu Zhang, <b>Zhuo Su† </b> <span style="color: #888;">(Project Lead)</span>, Xiaozheng Zheng, Zheng Lv, Guidong Wang, Yongjie Zhang, Qi Wu, Lei Chu, <a href="https://scholar.google.com/citations?user=Vm7d2EkAAAAJ&hl" style="color: #007bff;"> Ling Pei‡</a></span>
                        <br />
                        <span style="font-size: 14px; color: #444;"><i><b>CVPR 2025</b></i></span>
                        <br />
                        <p style="font-size: 14px; color: #444;">
                            We propose EnvPoser, a two-stage method using sparse tracking signals and pre-scanned environment from VR devices to perform full-body motion estimation and handle the multi-hypothesis nature with uncertainty-aware and environmental constraint integration.
                        </p>
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="https://arxiv.org/pdf/2412.10235" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="https://xspc.github.io/EnvPoser/" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />
                </td>
            </tr>
        </table>
    </li>


    <li class="paper-item" data-category="performance-capture animatable-avatar neural-rendering">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="./"><img src="./Files/reperformer.png" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance</b>
                        <br />
                        <span style="font-size: 14px; color: #444;"><a href="https://nowheretrix.github.io/" style="color: #007bff;">Yuheng Jiang</a>, Zhehao Shen, Chengcheng Guo, Yu Hong, <b>Zhuo Su</b>, Yingliang Zhang, <a href="https://people.mpi-inf.mpg.de/~mhaberma/" style="color: #007bff;">Marc Habermann‡</a>, <a href="http://www.xu-lan.com/" style="color: #007bff;">Lan Xu‡</a></span>
                        <br />
                        <span style="font-size: 14px; color: #444;"><i><b>CVPR 2025</b></i></span>
                        <br />
                        <p style="font-size: 14px; color: #444;">
                            We present RePerformer, a Gaussian-based representation for high-fidelity volumetric video playback and re-performance. Via Morton-based parameterization, our method enables efficient rendering. A semantic-aware alignment module and deformation transfer enhance realistic motion re-performance.
                        </p>
                        <div style="margin-top: 10px;">
                        <a href="https://arxiv.org/pdf/2503.12242" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="https://moqiyinlun.github.io/Reperformer/" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />
                </td>
            </tr>
        </table>
    </li>
    

    <li class="paper-item" data-category="motion-capture">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="./"><img src="./Files/EMHI.png" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>EMHI: A Multimodal Egocentric Human Motion Dataset with HMD and Body-Worn IMUs</b>
                        <br />
                        <p style="margin: 5px 0; font-size: 14px; color: #444;">Zhen Fan*, Peng Dai*, <b>Zhuo Su*</b>, Xu Gao, Zheng Lv, Jiarui Zhang, Tianyuan Du, Guidong Wang, Yang Zhang
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            <i><b>AAAI 2025</b></i>
                        </p>
                        <p style="font-size: 14px; color: #444;">
                            We introduce EMHI, a dataset combining stereo images from headsets and IMU data for egocentric human motion capture in VR. It includes 28.5 hours of data from 58 subjects. We also propose MEPoser, a method that effectively uses this multimodal data for improved pose estimation.
                        </p>
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="https://arxiv.org/pdf/2408.17168" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="https://pico-ai-team.github.io/EMHI/" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />
                </td>
            </tr>
        </table>
    </li>

    <li class="paper-item" data-category="animatable-avatar neural-rendering">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="#"><img src="./Files/headgap.png" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors</b>
                        <br />
                        <p style="margin: 5px 0; font-size: 14px; color: #444;">
                            <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Xiaozheng Zheng</a>, Chao Wen, Zhaohu Li, Weiyi Zhang, <b>Zhuo Su</b>, Xu Chang, Yang Zhao, Zheng Lv, Xiaoyuan Zhang, Yongjie Zhang, Guidong Wang, <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            <i><b>3DV 2025</b></i>
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            We propose a 3D head avatar creation method that generalizes from few-shot in-the-wild data. By using 3D head priors from a large-scale dataset and a Gaussian Splatting-based network, our approach achieves high-fidelity rendering and robust animation.
                        </p>
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="https://arxiv.org/pdf/2408.06019v1" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="https://headgap.github.io/" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />
                </td>
            </tr>
        </table>
    </li>
    
    <li class="paper-item" data-category="generation neural-rendering">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="#"><img src="./Files/humansplat.jpg" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors</b>
                        <br />
                        <p style="margin: 5px 0; font-size: 14px; color: #444;">
                            Panwang Pan*, <b>Zhuo Su* † </b> <span style="color: #888;">(Project Lead)</span>, Chenguo Lin*, 
                            Zhen Fan, Yongjie Zhang, Zeming Li, Tingting Shen, Yadong Mu, <a href="https://www.liuyebin.com/" style="color: #007bff";>Yebin Liu‡</a>
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            <i><b>NeurIPS 2024</b></i>
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            We propose HumanSplat, a method that predicts the 3D Gaussian Splatting properties of a human from a single input image in a generalizable way. It utilizes a 2D multi-view diffusion model and a latent reconstruction transformer with human structure priors to effectively integrate geometric priors and semantic features.
                        </p>
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="https://arxiv.org/pdf/2406.12459" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="https://humansplat.github.io/" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />                       
                </td>
            </tr>
        </table>
    </li>


    <li class="paper-item" data-category="performance-capture neural-rendering">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="./"><img src="./Files/Hifi4G.png" alt="HiFi4G: High-Fidelity Human Performance Rendering" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting</b>
                        <br />
                        <p style="margin: 5px 0; font-size: 14px; color: #444;">
                            <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Yuheng Jiang</a>, <a href="#" style="color: #007bff;">Zhehao Shen</a>, <a href="#" style="color: #007bff;">Penghao Wang</a>, <b>Zhuo Su</b>, <a href="#" style="color: #007bff;">Yu Hong</a>, <a href="#" style="color: #007bff;">Yingliang Zhang</a>, <a href="https://scholar.google.com/citations?user=R9L_AfQAAAAJ&hl">Jingyi Yu</a>, <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            <i><b>CVPR 2024.</b></i>
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            We present an explicit and compact Gaussian-based approach for high-fidelity human performance rendering from dense footage, in which our core intuition is to marry the 3D Gaussian representation with non-rigid tracking.
                        </p>
                        <div style="margin-top: 10px;">
                            <a href="https://arxiv.org/pdf/2312.03461.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                            <a href="https://nowheretrix.github.io/HiFi4G/" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px; margin-left: 10px;">Project page</a>
                        </div>
                        <br />
                    </p>
                </td>
            </tr>
        </table>
    </li>    
    
    <li class="paper-item" data-category="generation">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="#"><img src="./Files/joint2human.png" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>Joint2Human: High-quality 3D Human Generation via Compact Spherical Embedding of 3D Joints</b>
                        <br />
                        <p style="margin: 5px 0; font-size: 14px; color: #444;">
                            Muxin Zhang, Qiao Feng, <b>Zhuo Su</b>, Chao Wen, Zhou Xue, Kun Li
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            <i><b>CVPR 2024</b></i>
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            We introduce Joint2Human, a novel method that leverages 2D diffusion models to generate detailed 3D human geometry directly, ensuring both global structure and local details.
                        </p>
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Joint2Human_High-Quality_3D_Human_Generation_via_Compact_Spherical_Embedding_of_CVPR_2024_paper.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="https://cic.tju.edu.cn/faculty/likun/projects/Joint2Human/index.html" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />
                </td>
            </tr>
        </table>
    </li>

    <li class="paper-item" data-category="animatable-avatar">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="#"><img src="./Files/ohta.png" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>OHTA: One-shot Hand Avatar via Data-driven Implicit Priors</b>
                        <br />
                        <p style="margin: 5px 0; font-size: 14px; color: #444;">
                            <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Xiaozheng Zheng</a>, Chao Wen, <b>Zhuo Su</b>, Zeran Xu, Zhaohu Li, Yang Zhao, Zhou Xue
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            <i><b>CVPR 2024</b></i>
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            OHTA is a novel approach capable of creating implicit animatable hand avatars using just a single image. It facilitates 1) text-to-avatar conversion, 2) hand texture and geometry editing, and 3) interpolation and sampling within the latent space.
                        </p>
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_OHTA_One-shot_Hand_Avatar_via_Data-driven_Implicit_Priors_CVPR_2024_paper.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="https://github.com/bytedance/OHTA" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />
                </td>
            </tr>
        </table>
    </li>

    <li class="paper-item" data-category="motion-capture">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="./"><img src="./Files/hmdposer.png" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>HMD-Poser: On-Device Real-time Human Motion Tracking from Scalable Sparse Observations</b>
                        <br />
                        <p style="margin: 5px 0; font-size: 14px; color: #444;">
                            Peng Dai, Yang Zhang, Tao Liu, Zhen Fan, Tianyuan Du, <b>Zhuo Su</b>, Xiaozheng Zheng, Zeming Li
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            <i><b>CVPR 2024</b></i>
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            We propose HMD-Poser, the first unified approach to recover full-body motions using scalable sparse observations from HMD and body-worn IMUs. In particular, it can support a variety of input scenarios, such as HMD, HMD+2IMUs, HMD+3IMUs, etc.
                        </p>
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Dai_HMD-Poser_On-Device_Real-time_Human_Motion_Tracking_from_Scalable_Sparse_Observations_CVPR_2024_paper.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="https://pico-ai-team.github.io/hmd-poser" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />
                </td>
            </tr>
        </table>
    </li>

    <li class="paper-item" data-category="motion-capture">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="./"><img src="./Files/AvatarJLM.png" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>Realistic Full-Body Tracking from Sparse Observations via Joint-Level Modeling</b>
                        <br />
                        <p style="margin: 5px 0; font-size: 14px; color: #444;">
                            <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Xiaozheng Zheng</a>*, <b>Zhuo Su*</b>, Chao Wen, Zhou Xue, Xiaojie Jin
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            <i><b>ICCV 2023</b></i>
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            We propose a two-stage framework that can obtain accurate and smooth full-body motions with the three tracking signals of head and hands only, in which we first explicitly model the joint-level features and then utilize them as spatiotemporal transformer tokens to capture joint-level correlations.
                        </p>
                        <div style="margin-top: 10px;">
                            <a href="https://arxiv.org/pdf/2308.08855.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                            <a href="https://github.com/zxz267/AvatarJLM/" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                        </div>
                        <br />
                    </p>
                </td>
            </tr>
        </table>
    </li>
    
    <li class="paper-item" data-category="performance-capture neural-rendering">
        <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
            <tr>
                <td style="width: 30%; text-align: center;">
                    <a href="./"><img src="./Files/instant_nvr.png" alt="" style="width: 100%; height: auto;" /></a>
                </td>
                <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                    <p>
                        <b>Instant-NVR: Instant Neural Volumetric Rendering for Human-object Interactions from Monocular RGBD Stream</b>
                        <br />
                        <p style="margin: 5px 0; font-size: 14px; color: #444;">
                            Yuheng Jiang, Kaixin Yao, <b>Zhuo Su</b>, Zhehao Shen, Haimin Luo, <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            <i><b>CVPR 2023</b></i>
                        </p>
                        <p style="font-size: 14px; color: #444; margin-top: 10px;">
                            We propose a neural approach for instant volumetric human-object tracking and rendering using a single RGBD camera. It bridges traditional non-rigid tracking with recent instant radiance field techniques via a multi-thread tracking-rendering mechanism.
                        </p>
                        <div style="margin-top: 10px;">
                            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Instant-NVR_Instant_Neural_Volumetric_Rendering_for_Human-Object_Interactions_From_Monocular_CVPR_2023_paper.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                            <a href="https://nowheretrix.github.io/Instant-NVR/" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                        </div>
                        <br />
                    </p>
                </td>
            </tr>
        </table>
    </li>

    <li class="paper-item" data-category="performance-capture">
    <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
        <tr>
            <td style="width: 30%; text-align: center;">
                <a href="./"><img src="./Files/robustfusionplus.jpg" alt="" style="width: 100%; height: auto;" /></a>
            </td>
            <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                <p>
                    <b>Robust Volumetric Performance Reconstruction under Human-object Interactions from Monocular RGBD Stream</b>
                    <br />
                    <p style="margin: 5px 0; font-size: 14px; color: #444;">
                        <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>, Dawei Zhong, Zhong Li, Fan Deng, Shuxue Quan, <a href="http://luvision.net/" style="color: #007bff;">Lu Fang</a>
                    </p>
                    <p style="font-size: 14px; color: #444; margin-top: 10px;">
                        <i><b>TPAMI 2022</b></i>
                    </p>
                    <p style="font-size: 14px; color: #444; margin-top: 10px;">
                        We propose a robust volumetric performance reconstruction system for human-object interaction scenarios using only a single RGBD sensor, which combines various data-driven visual and interaction cues to handle the complex interaction patterns and severe occlusions.
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="./Projects/RobustFusion_plus_page/RobustFusionPlus.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="./Projects/RobustFusion_plus_page/index.html" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />
                </p>
            </td>
        </tr>
    </table>
</li>

<li class="paper-item" data-category="performance-capture neural-rendering">
    <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
        <tr>
            <td style="width: 30%; text-align: center;">
                <a href="./"><img src="./Files/neuralrendering.jpg" alt="NeuralHOFusion" style="width: 100%; height: auto;" /></a>
            </td>
            <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                <p>
                    <b>NeuralHOFusion: Neural Volumetric Rendering Under Human-Object Interactions</b>
                    <br />
                    <p style="margin: 5px 0; font-size: 14px; color: #444;">
                        Yuheng Jiang, Suyi Jiang, Guoxing Sun, <b>Zhuo Su</b>, Kaiwen Guo, Minye Wu, <a href="https://scholar.google.com/citations?user=R9L_AfQAAAAJ&hl">Jingyi Yu</a>, <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>
                    </p>
                    <p style="font-size: 14px; color: #444; margin-top: 10px;">
                        <i><b>CVPR 2022</b></i>
                    </p>
                    <p style="font-size: 14px; color: #444; margin-top: 10px;">
                        We propose a robust neural volumetric rendering method for human-object interaction scenarios using 6 RGBD cameras, achieving layer-wise and photorealistic reconstruction results of human performance in novel views.
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_NeuralHOFusion_Neural_Volumetric_Rendering_Under_Human-Object_Interactions_CVPR_2022_paper.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; margin-right: 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="https://nowheretrix.github.io/neuralfusion/" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Project page</a>
                    </div>
                    <br />
                </p>
            </td>
        </tr>
    </table>
</li>

<li class="paper-item" data-category="motion-capture">
    <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
        <tr>
            <td style="width: 30%; text-align: center;">
                <a href="./"><img src="./Files/stylemotion.png" alt="Learning Variational Motion Prior" style="width: 100%; height: auto;" /></a>
            </td>
            <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                <p>
                    <b>Learning Variational Motion Prior for Video-based Motion Capture</b>
                    <br />
                    <p style="margin: 5px 0; font-size: 14px; color: #444;">
                        Xin Chen*, <b>Zhuo Su*</b>, Lingbo Yang*, Pei Cheng, <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>, Gang Yu 
                        <span style="font-size: 12px; color: #777;"></span>
                    </p>
                    <p style="font-size: 14px; color: #444; margin-top: 10px;">
                        <i><b>arXiv 2022</b></i>
                    </p>
                    <p style="font-size: 14px; color: #444; margin-top: 10px;">
                        We propose a novel variational motion prior (VMP) learning approach for video-based motion capture. Specifically, VMP is implemented as a transformer-based variational autoencoder pretrained over large-scale 3D motion data, providing an expressive latent space for human motion at sequence level.
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="https://arxiv.org/pdf/2210.15134.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                    </div>
                    <br />
                </p>
            </td>
        </tr>
    </table>
</li>
 
<li class="paper-item" data-category="performance-capture">
    <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
        <tr>
            <td style="width: 30%; text-align: center;">
                <a href="./"><img src="./Files/robustfusion.jpg" alt="RobustFusion: Human Volumetric Capture" style="width: 100%; height: auto;" /></a>
            </td>
            <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                <p>
                    <b>RobustFusion: Human Volumetric Capture with Data-driven Visual Cues using a RGBD Camera</b>
                    <br />
                    <p style="margin: 5px 0; font-size: 14px; color: #444;">
                        <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>, <a href="https://zhengzerong.github.io/" style="color: #007bff;">Zerong Zheng</a>, <a href="https://ytrock.com/" style="color: #007bff;">Tao Yu</a>, <a href="https://liuyebin.com/" style="color: #007bff;">Yebin Liu</a>, <a href="http://luvision.net/" style="color: #007bff;">Lu Fang</a>
                    </p>
                    <p style="font-size: 14px; color: #444; margin-top: 10px;">
                        <i><b>ECCV 2020 (Spotlight)</b></i>
                    </p>
                    <p style="font-size: 14px; color: #444; margin-top: 10px;">
                        We introduce a robust human volumetric capture approach combined with various data-driven visual cues using a Kinect, which outperforms existing state-of-the-art approaches significantly.
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="./Files/Rofusion.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="./Projects/RobustFusion_page/index.html" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px; margin-left: 10px;">Project page</a>
                    </div>
                    <br />
                </p>
            </td>
        </tr>
    </table>
</li>

<li class="paper-item" data-category="performance-capture">
    <table class="imgtable" style="margin-left: 10px; margin-right: 10px; width: 100%; table-layout: fixed;">
        <tr>
            <td style="width: 30%; text-align: center;">
                <a href="./"><img src="./Files/unfusion.jpg" alt="UnstructuredFusion: Realtime 4D Geometry and Texture Reconstruction" style="width: 100%; height: auto;" /></a>
            </td>
            <td style="width: 70%; padding-left: 20px; vertical-align: top;">
                <p>
                    <b>UnstructuredFusion: Realtime 4D Geometry and Texture Reconstruction using Commercial RGBD Cameras</b>
                    <br />
                    <p style="margin: 5px 0; font-size: 14px; color: #444;">
                        <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>, <b>Zhuo Su</b>, <a href="https://sites.google.com/view/lhanaf/%E9%A6%96%E9%A1%B5" style="color: #007bff;">Lei Han</a>, <a href="https://ytrock.com/" style="color: #007bff;">Tao Yu</a>, <a href="https://liuyebin.com/" style="color: #007bff;">Yebin Liu</a>, <a href="http://luvision.net/" style="color: #007bff;">Lu Fang</a>
                    </p>
                    <p style="font-size: 14px; color: #444; margin-top: 10px;">
                        <i><b>TPAMI 2019</b></i>
                    </p>
                    <p style="font-size: 14px; color: #444; margin-top: 10px;">
                        We propose UnstructuredFusion, which allows realtime, high-quality, complete reconstruction of 4D textured models of human performance via only three commercial RGBD cameras.
                    </p>
                    <div style="margin-top: 10px;">
                        <a href="./Files/Unfusion.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Paper</a>
                        <a href="./Projects/UnstructedFusion_page/index.html" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px; margin-left: 10px;">Project page</a>
                    </div>
                </p>
            </td>
        </tr>
    </table>
</li>

<A NAME="Talks"><h2 style="font-size: 1.6em;">Invited Talks</h2></A>

<ul style="list-style-type: none; padding-left: 0;">
    <li style="margin-bottom: 20px; padding-left: 20px; vertical-align: top;">
        <div style="display: flex; align-items: center; gap: 20px;">
            <!-- Image container with width 30% and height auto to keep the aspect ratio -->
            <div style="width: 30%; text-align: center;">
                <a href="./">
                    <img src="./Files/talk-sg1.png" style="width: 98%; height: auto;" />
                </a>
            </div>
            <!-- Text container taking up the remaining 70% -->
            <div style="width: 70%; padding-left: 20px;">
                <p><b>Human-Centric Capture and Digitalization for Immersive XR Experiences
                </b></p>
                <p style="font-size: 14px; color: #444; margin-top: 10px;">This talk presents recent advances in human-centric capture and digitalization for XR, highlighting progress in motion capture, 3D reconstruction, performance capture, and avatar creation. We discuss how these pipelines work together to faithfully represent human motion, appearance, and behavior in virtual environments. By unifying these elements, we show how real people can be faithfully brought into immersive virtual worlds to enable true presence.
                </p>
                <p><i>Aug. 13, 2025, SIGGRAPH, Vancouver | Technical Workshop</i></p>
                </p>
                <div style="margin-top: 10px;">
                    <a href="./Files/siggraph-silde.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">Slide</a>
                    <a href="https://s2025.conference-schedule.org/presentation/?id=twork_102&sess=sess278" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px; margin-left: 10px;">Webpage</a>
                </div>
                </p>
            </div>
        </div>
    </li>
</ul>




<!-- <ul style="list-style-type: none; padding-left: 0;">
    <li style="margin-bottom: 20px; padding-left: 20px; vertical-align: top;">
        <div style="display: flex; align-items: center; gap: 20px;">
            <div style="width: 30%; text-align: center;">
                <a href="./">
                    <img src="./Files/talk-bytedance2.jpg" style="width: 98%; height: auto;" />
                </a>
            </div>
            <div style="width: 70%; padding-left: 20px;">
                <p><b>Revolutionizing XR Motion Capture: Sparse to Multi-Modal, Environment-Aware
                </b></p>
                <p style="font-size: 14px; color: #444; margin-top: 10px;">In XR, accurate full-body motion capture from sparse inputs is a key challenge. In this talk, we review four representative works — AvatarJLM, HMDPoser, EnvPoser, and EMHI — showcasing advances from sparse IMU modeling and egocentric sensing to multi-modal fusion and environment-aware inference, pushing sparse motion capture toward practical XR applications.
                </p>
                <p><i>Jun. 5, 2025, ByteDance, Online Live Stream | ByteTech Technical Sharing Seminar</i></p>
                </p>
                <div style="margin-top: 10px;">
                    <a href="./Files/human-recon-gen.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px; margin-left: 10px;">Slide</a>
                    <a href="https://mp.weixin.qq.com/s/wUIYdSyo3I97I9651uiSLA" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">WeChat Post</a>
                </div>
                </p>
            </div>
        </div>
    </li>
</ul> -->


<ul style="list-style-type: none; padding-left: 0;">
    <li style="margin-bottom: 20px; padding-left: 20px; vertical-align: top;">
        <div style="display: flex; align-items: center; gap: 20px;">
            <!-- Image container with width 30% and height auto to keep the aspect ratio -->
            <div style="width: 30%; text-align: center;">
                <a href="./">
                    <img src="./Files/china3dv.jpg" style="width: 98%; height: auto;" />
                </a>
            </div>
            <!-- Text container taking up the remaining 70% -->
            <div style="width: 70%; padding-left: 20px;">
                <p><b>Human Motion Capture and Avatar Creation using Sparse Observations
                </b></p>
                <p style="font-size: 14px; color: #444; margin-top: 10px;">The development of high-fidelity 3D human motion capture and avatar reconstruction is essential for immersive experiences in XR applications. This line of work explores modeling under sparse observation settings, tackling the challenges posed by limited sensors and minimal image inputs. The research spans from multi-modal motion understanding to generalizable avatar generation, and I share this exploration with the hope of contributing to the XR technologies.
                </p>
                <p><i>Apr. 11, 2025  China3DV, Beijing | Young Scholar Forum</i></p>
                </p>
                <div style="margin-top: 10px;">
                    <a href="http://china3dv.csig.org.cn/youngScholar.html#:~:text=PPT%E4%B8%8B%E8%BD%BD-,%E8%8B%8F%E5%8D%93%EF%BC%8C%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8,-%E4%BD%9C%E8%80%85%E7%AE%80%E4%BB%8B%EF%BC%9A%E8%8B%8F" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px; margin-left: 10px;">Webpage</a>
                    <a href="https://mp.weixin.qq.com/s/1_kiXFX5s0KaxspKaliJfA" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">WeChat Post</a>
                </div>
                </p>
            </div>
        </div>
    </li>
</ul>



<!-- <ul style="list-style-type: none; padding-left: 0;">
    <li style="margin-bottom: 20px; padding-left: 20px; vertical-align: top;">
        <div style="display: flex; align-items: center; gap: 20px;">
            <div style="width: 30%; text-align: center;">
                <a href="./">
                    <img src="./Files/talk-bytedance.png" style="width: 98%; height: auto;" />
                </a>
            </div>
            <div style="width: 70%; padding-left: 20px;">
                <p><b>Human 3D Reconstruction and Generation
                </b></p>
                <p style="font-size: 14px; color: #444; margin-top: 10px;">The construction of realistic 3D human avatars is crucial in VR/AR applications. This talk focuses on 3D human modeling, covering topics from traditional volumetric capture to neural rendering, from per-scene optimization to generalizable prior model training and generative methods. I shared my exploration in this field, hoping to inspire related research.
                </p>
                <p><i>Dec. 26, 2024, ByteDance, Online Live Stream | ByteTech Technical Sharing Seminar</i></p>
                </p>
                <div style="margin-top: 10px;">
                    <a href="./Files/human-recon-gen.pdf" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px; margin-left: 10px;">Slide</a>
                    <a href="https://mp.weixin.qq.com/s/wUIYdSyo3I97I9651uiSLA" style="border: 1px solid #007bff; color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">WeChat Post</a>
                </div>
                </p>
            </div>
        </div>
    </li>
</ul>
 -->


<div style="padding-left: 20px;">
    <p><b>ByteTech Technical Sharing Seminar Series</b></p>

    <!-- Talk 1 -->
    <div style="display: flex; align-items: center; gap: 20px; margin-top: 20px;">
        <div style="width: 30%; text-align: center;">
            <a href="./">
                <img src="./Files/talk-bytedance.png" style="width: 98%; height: auto;" />
            </a>
        </div>
        <div style="width: 70%; padding-left: 20px;">
            <p><b>Human 3D Reconstruction and Generation</b></p>
            <p style="font-size: 14px; color: #444; margin-top: 5px;">
                The rconstruction of realistic 3D human is crucial in VR/AR applications.
                This talk focuses on 3D human modeling, covering topics from traditional volumetric capture 
                to neural rendering, from per-scene optimization to generalizable prior model training and generative methods.
            </p>
            <p><i>Dec. 26, 2024, ByteDance, Online Live Stream</i></p>
            <div style="margin-top: 5px;">
                <a href="./Files/human-recon-gen.pdf" style="border: 1px solid #007bff; color: #007bff; 
                   padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px; margin-right: 10px;">Slide</a>
                <a href="https://mp.weixin.qq.com/s/wUIYdSyo3I97I9651uiSLA" style="border: 1px solid #007bff; 
                   color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">WeChat Post</a>
            </div>
        </div>
    </div>

    <!-- Talk 2 -->
    <div style="display: flex; align-items: center; gap: 20px; margin-top: 30px;">
        <div style="width: 30%; text-align: center;">
            <a href="./">
                <img src="./Files/talk-bytedance2.jpg" style="width: 98%; height: auto;" />
            </a>
        </div>
        <div style="width: 70%; padding-left: 20px;">
            <p><b>Revolutionizing XR MoCap: Sparse to Multi-Modal, Environment-Aware</b></p>
            <p style="font-size: 14px; color: #444; margin-top: 5px;">
                This talk reviews four representative works — AvatarJLM, HMDPoser, EnvPoser, and EMHI — 
                showcasing advances from sparse IMU modeling and egocentric sensing to multi-modal fusion 
                and environment-aware inference, pushing sparse motion capture toward practical XR applications.
            </p>
            <p><i>Jun. 5, 2025, ByteDance, Online Live Stream</i></p>
            <div style="margin-top: 5px;">
                <a href="./Files/XR-mocap.pdf" style="border: 1px solid #007bff; color: #007bff; 
                   padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px; margin-right: 10px;">Slide</a>
                <a href="https://mp.weixin.qq.com/s/PMzgRA9HdiGHMy_jsJZFSA" style="border: 1px solid #007bff; 
                   color: #007bff; padding: 5px 15px; text-decoration: none; border-radius: 4px; font-size: 14px;">WeChat Post</a>
            </div>
        </div>
    </div>
</div>





<A NAME="Awards"><h2 style="font-size: 1.6em;">Awards</h2></A>
<ul>
<li>PICO "Star Team Award" Innovation Breakthrough Award (<span style="font-style: italic;">创新突破奖&nbsp</span>), Bytedance, 2023</li>
<li>Tencent Open Source Collaboration Award (<span style="font-style: italic;">腾讯开源协同奖&nbsp</span>), Tencent, 2021</li>
<li>Outstanding Graduate of Beijing, Beijing, 2021</li>
<li>Outstanding Graduate of Department of Automation, Tsinghua University, 2021</li>
<li>Excellent Bachelor Thesis Award, Northeastern University, 2018</li>
<li>Outstanding Graduate of Liaoning Province, Liaoning Province, 2018</li>
<li>National Scholarship, Ministry of Education, 2018</li>
<li>Excellence Award for National Undergraduate Innovation Program, Northeastern University, 2017</li>
<li>City's Excellent Undergraduate, Shenyang City, 2017</li>
<li>Mayor's Scholarship, Shenyang City, 2017</li>
<li>Top Ten Excellent Undergraduate (10 / the whole university, <span style="font-style: italic;">十佳本科生&nbsp</span>), Northeastern University, 2017</li>
<li>Honorable Mention of American Mathematical Contest in Modeling, COMAP, 2017</li>
<li>Second Prize of National Undergraduate Mathematical Contest in Modeling, CSIAM, 2016</li>
<li>First Prize of Provincial Undergraduate Mathematical Contest in Modeling, Liaoning Province, 2016</li>
<li>2x Second Prize of Electronic Design Contest, Education Department of Liaoning Province, 2015-2016</li>
<li>4x First Class Scholarships, Northeastern University, 2015-2018</li>
</ul>
<br />


<A NAME="Miscellaneous"><h2 style="font-size: 1.6em;">Miscellaneous</h2></A>
    <ul>
    <li> Application of motion capture technology in real-world products: <a href="https://www.picoxr.com/global/products/pico-motion-tracker" style="color: #007bff;">Pico Motion Tracker</a>, a cutting-edge solution for precise motion tracking in virtual reality.</li>
    <li> 7 patents related to motion capture, avatar creation, and related technologies have been filed at Bytedance.</li>

    <li> Xiaozheng Zheng, <b>Zhuo Su</b>, Chao Wen, Zhou Xue, "Method and apparatus for pose estimation, and electronic device", Patent, US20250028385A1</li>
    
    <!-- <hr style="border: 1px solid #ddd; width: 80%; margin-left: 0; margin-top: 10px; margin-bottom: 10px;"> -->

    <li> <a href="http://luvision.net/" style="color: #007bff;">Lu Fang</a>, <a href="https://ji-mengqi.com/" style="color: #007bff;">Mengqi Ji</a>, <a href="https://sites.google.com/view/lhanaf/%E9%A6%96%E9%A1%B5" style="color: #007bff;">Lei Han</a>, <b>Zhuo Su</b>,  <a href="https://media.au.tsinghua.edu.cn/info/1009/1112.htm" style="color: #007bff;"> Qionghai Dai</a>,
        “Depth Camera-based three-dimensional renconstruction method and apparatus, device, and storage medium”, Patent, US201916977899A
   </li>
    <li> <a href="http://luvision.net/" style="color: #007bff;">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://sites.google.com/view/lhanaf/%E9%A6%96%E9%A1%B5" style="color: #007bff;">Lei Han</a>, <a href="https://media.au.tsinghua.edu.cn/info/1009/1112.htm" style="color: #007bff;"> Qionghai Dai</a>,
         “Depth camera calibration method and device, electronic equipment and storage medium”, Patent, CN:201810179738:A
    </li>
    <li>
        <a href="http://luvision.net/" style="color: #007bff;">Lu Fang</a>, <a href="https://sites.google.com/view/lhanaf/%E9%A6%96%E9%A1%B5" style="color: #007bff;">Lei Han</a>, <b>Zhuo Su</b>, <a href="https://media.au.tsinghua.edu.cn/info/1009/1112.htm" style="color: #007bff;"> Qionghai Dai</a>,
        “A three-dimensional rebuilding method and device based on a depth camera, an apparatus and a storage medium”, Patent, CN:201810179264:A
    </li>
    <li>
        <a href="http://luvision.net/" style="color: #007bff;">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>,
        “Dynamic three-dimensional reconstruction method, device, equipment, medium and system”, Patent, CN:201910110062:A
    </li>
    <li>
        <a href="http://luvision.net/" style="color: #007bff;">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>,
        “Texture real-time determination method, device and equipment for dynamic scene and medium”, Patent, CN:201910110044:A
    </li>
    <li>
        <a href="http://luvision.net/" style="color: #007bff;">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>, Jianwei Wen, Chao Yuan,
        “Dynamic human body three-dimensional reconstruction method, device, equipment and medium”, Patent, CN:202010838902:A
    </li>
    <li>
        <a href="http://luvision.net/" style="color: #007bff;">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html" style="color: #007bff;">Lan Xu</a>, Jianwei Wen, Chao Yuan,
        “Dynamic human body three-dimensional model completion method and device, equipment and medium”, Patent, CN:202010838890:A
    </li>

    <!-- <hr style="border: 1px solid #ddd; width: 80%; margin-left: 0; margin-top: 10px; margin-bottom: 10px;"> -->
    <li>
        <b>Zhuo Su</b>, Xiaozhe Wang, Wen Fei, Changfu Zhou,
        “Multi-feature information landmark detection method for precise landing of unmanned aerial vehicle”, Patent, CN:201710197369:A
    </li>

    <li> Wen Fei, <b>Zhuo Su‡</b> <span style="color: #888;">(Corresponding author)</span>, Changfu Zhou,
         “Artificial landmark design and detection using hierarchy information for UAV localization and landing”,
    Conference Paper, Chinese Control And Decision Conference (CCDC) 2017,
        <!-- [<a href="./Files/UAV.pdf">Paper</a>] -->
    </li>
    <li>Haina Wu, <b>Zhuo Su</b>, Kai Luo, Qi Wang, Xianzhong Cheng
        "Exploration and Research on the Movement of Magnus Glider”, Journal Paper, Physical Experiment of College, 2015 (5): 2
        <!--[<a href="./Files/Glider.pdf">Paper</a>]-->
    </li>
</ul>
<br />

<!-- <A NAME="Skills"><h2>Skills</h2></A>
&nbsp;&nbsp;&nbsp;&nbsp; C & C++(OpenCV, OpenGL, CUDA, Eigen, ...), Python(Pytorch), Matlab, LaTeX, ...
<br />

<A NAME="Services"><h2>Services</h2></A>
&nbsp;&nbsp;&nbsp;&nbsp; Reviewer for NeurIPS, CVPR, ICCV, ICLR, ICML, TVCG, IEEEVR, 3DV, AISTATS, ...
<br /> -->


<br />

<!--<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5vmure15sa4&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
-->
</body>
</html>
